{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 게임 환경 로딩\n",
    "\n",
    "그리드의 크기는 자유롭게 조절하기 바랍니다. 크기를 작게 하면 우리의 DQN 에이전트가 보다 쉽게 작업을 수행할 것이며, 반대로 크게 하면 어려움을 배가시킬 것입니다. \n",
    "\n",
    "(역자주 : gridworld.py 라는 파일이 동일한 경로 내에 존재해야 합니다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrboKdF67g1sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113ba6cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gridworld import gameEnv\n",
    "\n",
    "env = gameEnv(partial=False,size=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 이미지는 우리의 간단한 게임 내의 시작 환경의 예시입니다. \n",
    "\n",
    "에이전트는 파란색 사각형을 위, 아래, 왼쪽, 오른쪽으로 이동시킵니다. 목표는 빨간색 사각형 (-1의 보상)을 피하여 녹색 사각형 (+1의 보상)까지 도달하는 것입니다. 세가지 블록의 위치는 매 에피소드마다 랜덤하게 변하게 됩니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네트워크 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,h_size):\n",
    "        #네트워크는 게임으로부터 하나의 프레임을 받아 이를 배열로 만든다 (flattening).\n",
    "        #배열의 크기를 재조절해주고 4개의 컨벌루션 레이어를 거치면서 처리해 준다.\n",
    "        self.scalarInput =  tf.placeholder(shape=[None,21168],dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,84,84,3])\n",
    "        self.conv1 = slim.conv2d( \\\n",
    "            inputs=self.imageIn,num_outputs=32,kernel_size=[8,8],stride=[4,4],padding='VALID', biases_initializer=None)\n",
    "        self.conv2 = slim.conv2d( \\\n",
    "            inputs=self.conv1,num_outputs=64,kernel_size=[4,4],stride=[2,2],padding='VALID', biases_initializer=None)\n",
    "        self.conv3 = slim.conv2d( \\\n",
    "            inputs=self.conv2,num_outputs=64,kernel_size=[3,3],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        self.conv4 = slim.conv2d( \\\n",
    "            inputs=self.conv3,num_outputs=h_size,kernel_size=[7,7],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        \n",
    "        #마지막 컨벌루션 레이어로부터의 출력값을 취한 후, 이를 어드밴티지 스트림과 값 스트림으로 분리한다. \n",
    "        self.streamAC,self.streamVC = tf.split(self.conv4,2,3)\n",
    "        self.streamA = slim.flatten(self.streamAC)\n",
    "        self.streamV = slim.flatten(self.streamVC)\n",
    "        self.AW = tf.Variable(tf.random_normal([h_size//2,env.actions]))\n",
    "        self.VW = tf.Variable(tf.random_normal([h_size//2,1]))\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        #최종 Q-값을 얻기 위해 어드밴티지 스트림과 값 스트림을 조합해 준다. \n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        #타겟 Q 값과 예측 Q 값 간의 제곱합 차를 취함으로써 비용을 구한다.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,env.actions,dtype=tf.float32)\n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        \n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 경험 리플레이\n",
    "\n",
    "아래 클래스는 경험과 샘플을 저장하고 랜덤하게 네트워크를 학습시킵니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self,size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 게임의 프레임의 사이즈를 조절해 주는 간단한 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processState(states):\n",
    "    return np.reshape(states,[21168])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 함수들은 1차 네트워크의 파라미터와 함께 타겟 네트워크의 파라미터를 업데이트하게 해줍니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네트워크 학습\n",
    "\n",
    "모든 학습 파라미터를 설정합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32 #각 학습 단계에서 사용할 경험의 수\n",
    "update_freq = 4 #학습 단계 기준의 업데이트 주기 \n",
    "y = .99 #타겟 Q-값에 대한 할인 계수\n",
    "startE = 1 #시작 시 랜덤 액션의 가능성\n",
    "endE = 0.1 #종료 시 랜덤 액션의 가능성\n",
    "anneling_steps = 10000. #startE에서 endE로 줄어드는데 필요한 학습 단계 수\n",
    "num_episodes = 10000 #네트워크를 학습시키기 위한 게임 환경 에피소드의 수\n",
    "pre_train_steps = 10000 #학습 시작 전 랜덤 액션의 단계 수\n",
    "max_epLength = 50 #허용되는 최대 에피소드 길이\n",
    "load_model = False #저장된 모델을 로딩할 지 여부\n",
    "path = \"./dqn\" #모델을 저장할 경로\n",
    "h_size = 512 #어드밴티지 스트림과 값 스트림으로 분리되기 전의 마지막 컨벌루션 레이어의 크기\n",
    "tau = 0.001 #타겟 네트워크를 제1네트워크로 업데이트시켜 가는 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "500 0.7 1\n",
      "1000 1.6 1\n",
      "1500 3.0 1\n",
      "2000 2.3 1\n",
      "2500 1.5 1\n",
      "3000 2.8 1\n",
      "3500 2.3 1\n",
      "4000 2.1 1\n",
      "4500 2.7 1\n",
      "5000 0.7 1\n",
      "5500 1.7 1\n",
      "6000 2.3 1\n",
      "6500 1.3 1\n",
      "7000 2.2 1\n",
      "7500 3.0 1\n",
      "8000 2.5 1\n",
      "8500 3.7 1\n",
      "9000 1.9 1\n",
      "9500 2.3 1\n",
      "10000 1.3 1\n",
      "10500 3.3 0.9549999999999828\n",
      "11000 3.5 0.9099999999999655\n",
      "11500 1.7 0.8649999999999483\n",
      "12000 2.6 0.819999999999931\n",
      "12500 1.5 0.7749999999999138\n",
      "13000 3.0 0.7299999999998965\n",
      "13500 3.2 0.6849999999998793\n",
      "14000 0.8 0.639999999999862\n",
      "14500 1.6 0.5949999999998448\n",
      "15000 1.8 0.5499999999998275\n",
      "15500 1.5 0.5049999999998103\n",
      "16000 1.7 0.4599999999998177\n",
      "16500 1.4 0.41499999999982823\n",
      "17000 1.5 0.36999999999983874\n",
      "17500 1.3 0.32499999999984924\n",
      "18000 0.7 0.27999999999985975\n",
      "18500 0.4 0.23499999999986562\n",
      "19000 1.9 0.18999999999986225\n",
      "19500 1.3 0.14499999999985888\n",
      "20000 1.3 0.09999999999985551\n",
      "20500 0.2 0.09999999999985551\n",
      "21000 -0.3 0.09999999999985551\n",
      "21500 0.9 0.09999999999985551\n",
      "22000 0.8 0.09999999999985551\n",
      "22500 0.5 0.09999999999985551\n",
      "23000 1.7 0.09999999999985551\n",
      "23500 0.7 0.09999999999985551\n",
      "24000 0.0 0.09999999999985551\n",
      "24500 0.7 0.09999999999985551\n",
      "25000 0.6 0.09999999999985551\n",
      "25500 0.4 0.09999999999985551\n",
      "26000 1.0 0.09999999999985551\n",
      "26500 1.8 0.09999999999985551\n",
      "27000 1.4 0.09999999999985551\n",
      "27500 1.1 0.09999999999985551\n",
      "28000 1.3 0.09999999999985551\n",
      "28500 0.7 0.09999999999985551\n",
      "29000 0.3 0.09999999999985551\n",
      "29500 1.2 0.09999999999985551\n",
      "30000 1.3 0.09999999999985551\n",
      "30500 0.8 0.09999999999985551\n",
      "31000 0.6 0.09999999999985551\n",
      "31500 0.1 0.09999999999985551\n",
      "32000 1.2 0.09999999999985551\n",
      "32500 1.4 0.09999999999985551\n",
      "33000 1.0 0.09999999999985551\n",
      "33500 1.5 0.09999999999985551\n",
      "34000 0.9 0.09999999999985551\n",
      "34500 0.5 0.09999999999985551\n",
      "35000 1.1 0.09999999999985551\n",
      "35500 0.8 0.09999999999985551\n",
      "36000 1.2 0.09999999999985551\n",
      "36500 1.6 0.09999999999985551\n",
      "37000 1.7 0.09999999999985551\n",
      "37500 1.2 0.09999999999985551\n",
      "38000 0.8 0.09999999999985551\n",
      "38500 0.9 0.09999999999985551\n",
      "39000 2.1 0.09999999999985551\n",
      "39500 1.5 0.09999999999985551\n",
      "40000 1.8 0.09999999999985551\n",
      "40500 1.7 0.09999999999985551\n",
      "41000 1.6 0.09999999999985551\n",
      "41500 1.3 0.09999999999985551\n",
      "42000 1.2 0.09999999999985551\n",
      "42500 1.6 0.09999999999985551\n",
      "43000 0.7 0.09999999999985551\n",
      "43500 0.9 0.09999999999985551\n",
      "44000 1.8 0.09999999999985551\n",
      "44500 0.9 0.09999999999985551\n",
      "45000 1.1 0.09999999999985551\n",
      "45500 2.2 0.09999999999985551\n",
      "46000 2.3 0.09999999999985551\n",
      "46500 0.6 0.09999999999985551\n",
      "47000 1.2 0.09999999999985551\n",
      "47500 1.1 0.09999999999985551\n",
      "48000 1.7 0.09999999999985551\n",
      "48500 1.4 0.09999999999985551\n",
      "49000 0.9 0.09999999999985551\n",
      "49500 0.5 0.09999999999985551\n",
      "50000 1.4 0.09999999999985551\n",
      "Saved Model\n",
      "50500 1.8 0.09999999999985551\n",
      "51000 1.7 0.09999999999985551\n",
      "51500 2.3 0.09999999999985551\n",
      "52000 1.9 0.09999999999985551\n",
      "52500 1.0 0.09999999999985551\n",
      "53000 2.0 0.09999999999985551\n",
      "53500 1.4 0.09999999999985551\n",
      "54000 3.0 0.09999999999985551\n",
      "54500 2.5 0.09999999999985551\n",
      "55000 2.7 0.09999999999985551\n",
      "55500 1.1 0.09999999999985551\n",
      "56000 1.7 0.09999999999985551\n",
      "56500 1.2 0.09999999999985551\n",
      "57000 1.0 0.09999999999985551\n",
      "57500 3.7 0.09999999999985551\n",
      "58000 2.1 0.09999999999985551\n",
      "58500 2.3 0.09999999999985551\n",
      "59000 3.1 0.09999999999985551\n",
      "59500 2.4 0.09999999999985551\n",
      "60000 1.0 0.09999999999985551\n",
      "60500 2.1 0.09999999999985551\n",
      "61000 2.2 0.09999999999985551\n",
      "61500 1.7 0.09999999999985551\n",
      "62000 3.2 0.09999999999985551\n",
      "62500 2.3 0.09999999999985551\n",
      "63000 1.9 0.09999999999985551\n",
      "63500 1.3 0.09999999999985551\n",
      "64000 4.4 0.09999999999985551\n",
      "64500 1.7 0.09999999999985551\n",
      "65000 2.3 0.09999999999985551\n",
      "65500 3.9 0.09999999999985551\n",
      "66000 2.6 0.09999999999985551\n",
      "66500 2.6 0.09999999999985551\n",
      "67000 2.4 0.09999999999985551\n",
      "67500 1.5 0.09999999999985551\n",
      "68000 1.9 0.09999999999985551\n",
      "68500 2.1 0.09999999999985551\n",
      "69000 3.3 0.09999999999985551\n",
      "69500 2.1 0.09999999999985551\n",
      "70000 3.9 0.09999999999985551\n",
      "70500 3.0 0.09999999999985551\n",
      "71000 2.5 0.09999999999985551\n",
      "71500 3.0 0.09999999999985551\n",
      "72000 2.2 0.09999999999985551\n",
      "72500 4.4 0.09999999999985551\n",
      "73000 5.5 0.09999999999985551\n",
      "73500 4.0 0.09999999999985551\n",
      "74000 2.4 0.09999999999985551\n",
      "74500 3.4 0.09999999999985551\n",
      "75000 3.8 0.09999999999985551\n",
      "75500 2.8 0.09999999999985551\n",
      "76000 5.6 0.09999999999985551\n",
      "76500 2.7 0.09999999999985551\n",
      "77000 4.3 0.09999999999985551\n",
      "77500 5.0 0.09999999999985551\n",
      "78000 6.5 0.09999999999985551\n",
      "78500 2.5 0.09999999999985551\n",
      "79000 4.8 0.09999999999985551\n",
      "79500 2.9 0.09999999999985551\n",
      "80000 4.7 0.09999999999985551\n",
      "80500 4.0 0.09999999999985551\n",
      "81000 5.7 0.09999999999985551\n",
      "81500 5.9 0.09999999999985551\n",
      "82000 6.2 0.09999999999985551\n",
      "82500 4.0 0.09999999999985551\n",
      "83000 2.8 0.09999999999985551\n",
      "83500 4.1 0.09999999999985551\n",
      "84000 9.2 0.09999999999985551\n",
      "84500 7.5 0.09999999999985551\n",
      "85000 7.4 0.09999999999985551\n",
      "85500 6.5 0.09999999999985551\n",
      "86000 4.6 0.09999999999985551\n",
      "86500 4.8 0.09999999999985551\n",
      "87000 6.1 0.09999999999985551\n",
      "87500 8.1 0.09999999999985551\n",
      "88000 8.8 0.09999999999985551\n",
      "88500 9.3 0.09999999999985551\n",
      "89000 9.6 0.09999999999985551\n",
      "89500 7.8 0.09999999999985551\n",
      "90000 11.0 0.09999999999985551\n",
      "90500 6.7 0.09999999999985551\n",
      "91000 6.6 0.09999999999985551\n",
      "91500 11.2 0.09999999999985551\n",
      "92000 10.7 0.09999999999985551\n",
      "92500 12.7 0.09999999999985551\n",
      "93000 11.7 0.09999999999985551\n",
      "93500 10.5 0.09999999999985551\n",
      "94000 10.2 0.09999999999985551\n",
      "94500 8.2 0.09999999999985551\n",
      "95000 12.6 0.09999999999985551\n",
      "95500 13.2 0.09999999999985551\n",
      "96000 13.5 0.09999999999985551\n",
      "96500 12.8 0.09999999999985551\n",
      "97000 14.0 0.09999999999985551\n",
      "97500 12.3 0.09999999999985551\n",
      "98000 12.6 0.09999999999985551\n",
      "98500 12.7 0.09999999999985551\n",
      "99000 13.5 0.09999999999985551\n",
      "99500 14.3 0.09999999999985551\n",
      "100000 13.9 0.09999999999985551\n",
      "Saved Model\n",
      "100500 11.9 0.09999999999985551\n",
      "101000 15.5 0.09999999999985551\n",
      "101500 15.5 0.09999999999985551\n",
      "102000 13.2 0.09999999999985551\n",
      "102500 18.1 0.09999999999985551\n",
      "103000 14.9 0.09999999999985551\n",
      "103500 15.3 0.09999999999985551\n",
      "104000 14.9 0.09999999999985551\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-35f5b427e33d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m                     \u001b[0mtargetQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainBatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdoubleQ\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mend_multiplier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                     \u001b[0;31m#타겟 값을 이용해 네트워크를 업데이트\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateModel\u001b[0m\u001b[0;34m,\u001b[0m                         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalarInput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainBatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargetQ\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtargetQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrainBatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                     \u001b[0mupdateTarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetOps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#타겟 네트워크가 제1네트워크와 동일하도록 설정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tf_python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tf_python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tf_python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tf_python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tf_python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size)\n",
    "targetQN = Qnetwork(h_size)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "#랜덤 액션이 감소하는 비율을 설정 \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/anneling_steps\n",
    "\n",
    "#전체 보상과 에피소드 별 단계 수를 저장할 리스트를 생성\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "#모델이 저장될 경로 생성\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    updateTarget(targetOps,sess) #타겟 네트워크가 제1네트워크와 동일하도록 설정\n",
    "    for i in range(num_episodes):\n",
    "        episodeBuffer = experience_buffer()\n",
    "        #환경을 리셋하고 첫번째 관찰 얻기\n",
    "        s = env.reset()\n",
    "        s = processState(s)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        #Q-네트워크\n",
    "        while j < max_epLength: #만약 에이전트가 블록에 도달하기 위해 200회 이상 시도하면 종료\n",
    "            j+=1\n",
    "            # Q-네트워크로부터 (e의 확률로 랜덤한 액션과 함께) 그리디하게 액션을 선택한다.\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                a = np.random.randint(0,4)\n",
    "            else:\n",
    "                a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:[s]})[0]\n",
    "            s1,r,d = env.step(a)\n",
    "            s1 = processState(s1)\n",
    "            total_steps += 1\n",
    "            episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[1,5])) #에피소드 버퍼에 경험을 저장\n",
    "            \n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "                \n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    trainBatch = myBuffer.sample(batch_size) #경험에서 특정 부분을 랜덤하게 획득\n",
    "                    #타겟 Q-값에 대해 double DQN 업데이트를 수행\n",
    "                    Q1 = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    Q2 = sess.run(targetQN.Qout,feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    end_multiplier = -(trainBatch[:,4] - 1)\n",
    "                    doubleQ = Q2[range(batch_size),Q1]\n",
    "                    targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n",
    "                    #타겟 값을 이용해 네트워크를 업데이트\n",
    "                    _ = sess.run(mainQN.updateModel, \\\n",
    "                        feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]),mainQN.targetQ:targetQ, mainQN.actions:trainBatch[:,1]})\n",
    "                    \n",
    "                    updateTarget(targetOps,sess) #타겟 네트워크가 제1네트워크와 동일하도록 설정\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            \n",
    "            if d == True:\n",
    "\n",
    "                break\n",
    "        \n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "        #정기적으로 모델 저장\n",
    "        if i % 1000 == 0:\n",
    "            saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "            print(\"Saved Model\")\n",
    "        if len(rList) % 10 == 0:\n",
    "            print(total_steps,np.mean(rList[-10:]), e)\n",
    "    saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "print(\"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네트워크 학습 체크하기\n",
    "\n",
    "시간의 흐름에 따른 평균 보상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14be42278>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncUKFLiISJ1ToIiJxQoUuIhIn/g9+IlY1CKVbtAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115179ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rMat = np.resize(np.array(rList),[len(rList)//100,100])\n",
    "rMean = np.average(rMat,1)\n",
    "plt.plot(rMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
