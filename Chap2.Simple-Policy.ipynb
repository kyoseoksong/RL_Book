{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 밴딧 \n",
    "\n",
    "밴딧을 정의합니다. \n",
    "\n",
    "이 예에서는 4개의 손잡이를 가지는 밴딧을 이용하겠습니다. pullBandit 함수는 평균 값 0의 정규 분포로부터 랜덤한 숫자를 생성합니다. 밴딧의 수가 작을수록 +의 보상이 돌아올 가능성이 높습니다. 우리는 에이전트가 언제나 +의 보상을 가져올 손잡이를 선택하도록 학습하기를 원합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#밴딧의 손잡이 목록을 작성한다 \n",
    "#현재 손잡이 4 (인덱스 #3) 가 가장 자주 양의 보상을 제공하도록 설정\n",
    "\n",
    "bandit_arms = [0.2,0,-0.2,-2]\n",
    "num_arms = len(bandit_arms)\n",
    "def pullBandit(bandit):\n",
    "    #랜덤값을 구한다\n",
    "    result = np.random.randn(1)\n",
    "    if result > bandit:\n",
    "        #양의 보상을 반환한다\n",
    "        return 1\n",
    "    else:\n",
    "        #음의 보상을 반환한다\n",
    "        return -1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 에이전트 \n",
    "\n",
    "아래의 코드는 간단한 신경망을 구축합니다. \n",
    "\n",
    "이 신경망은 각각의 밴딧 손잡이에 대한 일련의 값들로 구성되어 있습니다. 각각의 값은 밴딧의 선택에 의해 반환되는 값의 추정치입니다. 우리는 정책 경사 방법을 이용하여 선택된 액션에 대한 값을 반환받은 보상 쪽으로 이동시켜 가는 식으로 에이전트를 업데이트시켜 나갑니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#네트워크의 피드-포워드 부분을 구축한다 \n",
    "weights = tf.Variable(tf.ones([num_arms]))\n",
    "output = tf.nn.softmax(weights)\n",
    "\n",
    "#학습 과정을 구축한다. \n",
    "#보상과 선택된 액션을 네트워크에 피드해 줌으로써 비용을 계산하고\n",
    "#비용을 이용해 네트워크를 업데이트한다\n",
    "reward_holder = tf.placeholder(shape=[1],dtype=tf.float32)\n",
    "action_holder = tf.placeholder(shape=[1],dtype=tf.int32)\n",
    "\n",
    "responsible_output = tf.slice(output,action_holder,[1])\n",
    "loss = -(tf.log(responsible_output)*reward_holder)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "update = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 에이전트 학습시키기 \n",
    "이제 우리는 우리의 환경 내에서 액션을 취함으로써 에이전트를 학습시키고 보상을 받게 됩니다. 보상과 액션을 이용함으로써 우리는 시간의 경과에 따라 최고의 보상을 받게 될 액션을 보다 자주 선택하기 위해 네트워크를 어떻게 업데이트시켜 나갈 지에 대해 알 수 있게 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward for the 4 arms of the bandit: [ 1.  0.  0.  0.]\n",
      "Running reward for the 4 arms of the bandit: [-3. -9.  8.  9.]\n",
      "Running reward for the 4 arms of the bandit: [  1.  -6.  14.  28.]\n",
      "Running reward for the 4 arms of the bandit: [ -4.  -4.  18.  45.]\n",
      "Running reward for the 4 arms of the bandit: [ -6.  -2.  22.  59.]\n",
      "Running reward for the 4 arms of the bandit: [-12.  -6.  26.  71.]\n",
      "Running reward for the 4 arms of the bandit: [-13.  -3.  22.  89.]\n",
      "Running reward for the 4 arms of the bandit: [ -15.   -8.   26.  100.]\n",
      "Running reward for the 4 arms of the bandit: [ -14.   -9.   31.  117.]\n",
      "Running reward for the 4 arms of the bandit: [  -9.   -7.   38.  133.]\n",
      "Running reward for the 4 arms of the bandit: [ -17.   -4.   36.  144.]\n",
      "Running reward for the 4 arms of the bandit: [ -24.   -2.   34.  159.]\n",
      "Running reward for the 4 arms of the bandit: [ -25.    5.   39.  170.]\n",
      "Running reward for the 4 arms of the bandit: [ -26.    4.   38.  191.]\n",
      "Running reward for the 4 arms of the bandit: [ -22.    4.   43.  202.]\n",
      "Running reward for the 4 arms of the bandit: [ -23.    4.   44.  212.]\n",
      "Running reward for the 4 arms of the bandit: [ -28.    1.   44.  232.]\n",
      "Running reward for the 4 arms of the bandit: [ -32.    4.   49.  250.]\n",
      "Running reward for the 4 arms of the bandit: [ -29.    2.   45.  265.]\n",
      "Running reward for the 4 arms of the bandit: [ -31.   -4.   47.  283.]\n",
      "\n",
      "The agent thinks arm 4 is the most promising....\n",
      "...and it was right!\n"
     ]
    }
   ],
   "source": [
    "total_episodes = 1000 #에이전트를 학습시킬 전체 에피소드 횟수 설정\n",
    "total_reward = np.zeros(num_arms) #밴딧의 손잡이에 대한 보상을 0으로 설정\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#텐서플로우 그래프를 론칭한다\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    while i < total_episodes:\n",
    "        \n",
    "        #볼츠만 분포에 따라 액션 선택\n",
    "        actions = sess.run(output)\n",
    "        a = np.random.choice(actions,p=actions)\n",
    "        action = np.argmax(actions == a)\n",
    "\n",
    "        reward = pullBandit(bandit_arms[action]) #밴딧 손잡이 중 하나를 선택함으로써 보상을 받는다\n",
    "        \n",
    "        #네트워크를 업데이트한다\n",
    "        _,resp,ww = sess.run([update,responsible_output,weights], feed_dict={reward_holder:[reward],action_holder:[action]})\n",
    "        \n",
    "        #보상의 총계 업데이트\n",
    "        total_reward[action] += reward\n",
    "        if i % 50 == 0:\n",
    "            print(\"Running reward for the \" + str(num_arms) + \" arms of the bandit: \" + str(total_reward))\n",
    "        i+=1\n",
    "print(\"\\nThe agent thinks arm \" + str(np.argmax(ww)+1) + \" is the most promising....\")\n",
    "if np.argmax(ww) == np.argmax(-np.array(bandit_arms)):\n",
    "    print(\"...and it was right!\")\n",
    "else:\n",
    "    print(\"...and it was wrong!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
